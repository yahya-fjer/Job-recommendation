{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aca181b9-74f5-49f1-8419-86c21e20a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee3e4a69-7251-4ac7-8b74-d42c1e1bdfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import udf, col, lower, desc, split, trim, expr\n",
    "from pyspark.sql.types import DoubleType, StringType, BooleanType, ArrayType\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, HashingTF, IDF, CountVectorizer\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbb38007-f2aa-4217-9a02-631cad79ef72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8750a1c5-5858-45ea-9b0d-f0643267ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "job_post_skills_path = \"linkedin_job_postings.csv\"   #sys.argv[1]\n",
    "stopwords_path = \"stopwords.txt\"  #sys.argv[2]\n",
    "job_seeker_path = \"test_cases.csv\"  #sys.argv[3]\n",
    "output_file_path = \"output_job_skills_match.csv\"  #sys.argv[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f372dd03-4f0e-47ab-b7eb-4ef2f0c30782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MatchJobSkills\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaa9251-7d0a-49e0-aacd-ef7b08cc27ce",
   "metadata": {},
   "source": [
    "## 2. Load Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3e3ad86-12aa-4d3d-9626-f4390ca974bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load posting & skills CSV into DataFrame\n",
    "df_posts_skills = spark.read.csv(job_post_skills_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab20d84c-9c57-4b3f-a4f3-64ae7e65f782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1348488\n",
      "root\n",
      " |-- job_link: string (nullable = true)\n",
      " |-- last_processed_time: string (nullable = true)\n",
      " |-- got_summary: string (nullable = true)\n",
      " |-- got_ner: string (nullable = true)\n",
      " |-- is_being_worked: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- job_location: string (nullable = true)\n",
      " |-- first_seen: string (nullable = true)\n",
      " |-- search_city: string (nullable = true)\n",
      " |-- search_country: string (nullable = true)\n",
      " |-- search_position: string (nullable = true)\n",
      " |-- job_level: string (nullable = true)\n",
      " |-- job_type: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+-----------+-------+---------------+--------------------+--------------------+--------------------+----------+-----------+--------------+--------------------+----------+--------+\n",
      "|            job_link| last_processed_time|got_summary|got_ner|is_being_worked|           job_title|             company|        job_location|first_seen|search_city|search_country|     search_position| job_level|job_type|\n",
      "+--------------------+--------------------+-----------+-------+---------------+--------------------+--------------------+--------------------+----------+-----------+--------------+--------------------+----------+--------+\n",
      "|https://www.linke...|2024-01-21 07:12:...|          t|      t|              f|Account Executive...|                  BD|       San Diego, CA|2024-01-15|   Coronado| United States|         Color Maker|Mid senior|  Onsite|\n",
      "|https://www.linke...|2024-01-21 07:39:...|          t|      t|              f|Registered Nurse ...|   Trinity Health MI|   Norton Shores, MI|2024-01-14|Grand Haven| United States|Director Nursing ...|Mid senior|  Onsite|\n",
      "|https://www.linke...|2024-01-21 07:40:...|          t|      t|              f|RESTAURANT SUPERV...|Wasatch Adaptive ...|           Sandy, UT|2024-01-14|     Tooele| United States|            Stand-In|Mid senior|  Onsite|\n",
      "|https://www.linke...|2024-01-21 07:40:...|          t|      t|              f|Independent Real ...|Howard Hanna | Ra...|Englewood Cliffs, NJ|2024-01-16|  Pinehurst| United States|   Real-Estate Clerk|Mid senior|  Onsite|\n",
      "|https://www.linke...|2024-01-19 09:45:...|          f|      f|              f|Group/Unit Superv...|IRS, Office of Ch...|        Chamblee, GA|2024-01-17|    Gadsden| United States|Supervisor Travel...|Mid senior|  Onsite|\n",
      "+--------------------+--------------------+-----------+-------+---------------+--------------------+--------------------+--------------------+----------+-----------+--------------+--------------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count number of rows\n",
    "num_rows = df_posts_skills.count()\n",
    "print(num_rows)\n",
    "\n",
    "# Show the DataFrame schema and some sample data\n",
    "df_posts_skills.printSchema()\n",
    "df_posts_skills.show(5)  # Show the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3f22e73-6070-4fdc-a37c-d669517ce8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(job_link='https://www.linkedin.com/jobs/view/on-demand-guest-advocate-cashier-general-merchandise-fulfillment-food-and-beverage-style-t2632-at-target-3734494804', last_processed_time='2024-01-21 00:38:44.231492+00', got_summary='t', got_ner='t', is_being_worked='f', job_title='On-Demand: Guest Advocate (Cashier), General Merchandise, Fulfillment, Food and Beverage, Style (T2632)', company='Target', job_location='Culver City, CA', first_seen='2024-01-12', search_city='Malibu', search_country='United States', search_position='Cashier Ii', job_level='Mid senior', job_type='Onsite')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts_skills.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95978c5-47e9-4161-8a17-1903ae1c1b6e",
   "metadata": {},
   "source": [
    "## 3. Pre-process the \"job_skills\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "047f546b-5543-404a-8522-f57e734949cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLTK objects\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords_nltk = list(stopwords.words(\"english\"))\n",
    "\n",
    "# Load stopwords from the file into a list\n",
    "with open(stopwords_path, \"r\") as f:\n",
    "    stopwords_txt = list(f.read().splitlines())\n",
    "\n",
    "# Combine stopwords from nltk & txt\n",
    "stop_words = set(stopwords_nltk + stopwords_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65bcb071-8ba9-4127-90bc-82b69b577c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean job_skills\n",
    "def clean_tokenize_skills(skills):\n",
    "    skills = re.sub(r'[^a-zA-Z,\\s]', '', skills) # Remove non-alphabetical characters except commas and spaces\n",
    "    skills = re.sub(r'\\s*,\\s*', ',', skills) # Remove extra spaces around commas\n",
    "    skills = re.sub(r'\\s+', ' ', skills)  # Remove extra spaces between words\n",
    "    skills = skills.lower() # Convert to lowercase\n",
    "    skills_list = skills.split(',') # Split skills by comma\n",
    "    cleaned_skills = []\n",
    "    for skill in skills_list:\n",
    "        skill = skill.strip() # Remove leading and trailing whitespace\n",
    "        if skill not in stop_words: # Remove stopwords\n",
    "            skill = lemmatizer.lemmatize(skill) # Lemmatize skill\n",
    "            skill = skill.replace(string.punctuation, '') # within each skill (element in the list), remove all punctuation including commas (as there should be no punctuation within each element)\n",
    "            cleaned_skills.append(skill)\n",
    "    return cleaned_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "920e9757-3df9-4a9a-accc-170d35fd3730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- job_link: string (nullable = true)\n",
      " |-- last_processed_time: string (nullable = true)\n",
      " |-- got_summary: string (nullable = true)\n",
      " |-- got_ner: string (nullable = true)\n",
      " |-- is_being_worked: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- job_location: string (nullable = true)\n",
      " |-- first_seen: string (nullable = true)\n",
      " |-- search_city: string (nullable = true)\n",
      " |-- search_country: string (nullable = true)\n",
      " |-- search_position: string (nullable = true)\n",
      " |-- job_level: string (nullable = true)\n",
      " |-- job_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_posts_skills.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aff8a3b2-091b-494f-b774-8df29d0de52b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `job_skills` cannot be resolved. Did you mean one of the following? [`job_level`, `job_link`, `job_title`, `job_type`, `got_ner`].;\n'Project [job_link#17, last_processed_time#18, got_summary#19, got_ner#20, is_being_worked#21, job_title#22, company#23, job_location#24, first_seen#25, search_city#26, search_country#27, search_position#28, job_level#29, job_type#30, clean_tokenize_skills('job_skills)#150 AS cleaned_job_skills#151]\n+- Relation [job_link#17,last_processed_time#18,got_summary#19,got_ner#20,is_being_worked#21,job_title#22,company#23,job_location#24,first_seen#25,search_city#26,search_country#27,search_position#28,job_level#29,job_type#30] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m clean_tokenize_udf \u001b[38;5;241m=\u001b[39m udf(clean_tokenize_skills, ArrayType(StringType()))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Apply UDF to clean and tokenize job_skills column\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m df_posts_skills_cleaned \u001b[38;5;241m=\u001b[39m \u001b[43mdf_posts_skills\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcleaned_job_skills\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_tokenize_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjob_skills\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:5176\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   5171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[0;32m   5172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   5173\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5174\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   5175\u001b[0m     )\n\u001b[1;32m-> 5176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `job_skills` cannot be resolved. Did you mean one of the following? [`job_level`, `job_link`, `job_title`, `job_type`, `got_ner`].;\n'Project [job_link#17, last_processed_time#18, got_summary#19, got_ner#20, is_being_worked#21, job_title#22, company#23, job_location#24, first_seen#25, search_city#26, search_country#27, search_position#28, job_level#29, job_type#30, clean_tokenize_skills('job_skills)#150 AS cleaned_job_skills#151]\n+- Relation [job_link#17,last_processed_time#18,got_summary#19,got_ner#20,is_being_worked#21,job_title#22,company#23,job_location#24,first_seen#25,search_city#26,search_country#27,search_position#28,job_level#29,job_type#30] csv\n"
     ]
    }
   ],
   "source": [
    "# Define UDF to apply cleaning and tokenization function\n",
    "clean_tokenize_udf = udf(clean_tokenize_skills, ArrayType(StringType()))\n",
    "\n",
    "# Apply UDF to clean and tokenize job_skills column\n",
    "df_posts_skills_cleaned = df_posts_skills.withColumn(\"cleaned_job_skills\", clean_tokenize_udf(col(\"job_skills\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40fb838-54ba-4f9b-b68d-87287cc7256c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
